{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd3e8ebce69d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Facebook_srossg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\""
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Facebook_srossg\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('compute-0-11', 27017)\n",
    "\n",
    "db = client.FacebookChallenge_akar1\n",
    "#train data\n",
    "collection = db.fb_hw\n",
    "contents = collection.find().limit(1000)\n",
    "#test data\n",
    "collection2 = db.fb_hw_test\n",
    "contents2 = collection2.find().limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bson import json_util, ObjectId\n",
    "import json\n",
    "rddSan = json.loads(json_util.dumps(contents))\n",
    "#test data\n",
    "rddSan2 = json.loads(json_util.dumps(contents2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(rddSan)\n",
    "#test data\n",
    "rdd2 = sc.parallelize(rddSan2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"Body\", StringType(), True), \n",
    "                     StructField(\"Id\", IntegerType(), True), \n",
    "                     StructField(\"Tags\", StringType(), True),\n",
    "                     StructField(\"Title\", StringType(), True),\n",
    "                     StructField(\"_id\", StringType(), True)])\n",
    "\n",
    "#test data schema\n",
    "schema2 = StructType([StructField(\"Body\", StringType(), True), \n",
    "                     StructField(\"Id\", IntegerType(), True), \n",
    "                     StructField(\"Title\", StringType(), True),\n",
    "                     StructField(\"_id\", StringType(), True)])\n",
    "train = sqlContext.createDataFrame(rdd, schema=schema)\n",
    "test = sqlContext.createDataFrame(rdd2, schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"_id\")\n",
    "#eliminate the duplicates\n",
    "train = train.drop_duplicates(['Body','Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the relevant packages\n",
    "from pyspark.sql import SQLContext, functions as sf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler,Tokenizer, RegexTokenizer,StopWordsRemover, StringIndexer, NGram, HashingTF, IDF,MinHashLSH,MinHashLSHModel,Word2Vec\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, OneVsRest,LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.printSchema(), test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv as csv \n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference:https://github.com/alexeyza/Kaggle-Facebook3/blob/master/model/pre_process.py\n",
    "def filter_html_tags(text):\n",
    "    # the following tags and their content will be removed, for example <a> tag will remove any html links\n",
    "    tags_to_filter = ['code','a']\n",
    "    if isinstance(text, unicode):\n",
    "        text = text.encode('utf8')\n",
    "    soup = BeautifulSoup(text)\n",
    "    for tag_to_filter in tags_to_filter:\n",
    "        text_to_remove = soup.findAll(tag_to_filter)\n",
    "        [tag.extract() for tag in text_to_remove]\n",
    "    return soup.get_text()\n",
    "filter_html_tags_udf = udf(filter_html_tags, StringType())\n",
    "train2 = train.withColumn(\"body\",filter_html_tags_udf(\"Body\"))\n",
    "test2 = test.withColumn(\"body\",filter_html_tags_udf(\"Body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitting the transformers\n",
    "tokenize_body  = RegexTokenizer(inputCol=\"body\",outputCol=\"body_tokenized\", pattern=\"[^a-zA-Z#+]\")\n",
    "train1 = tokenize_body.transform(train2)\n",
    "test1 = tokenize_body.transform(test2)\n",
    "tokenize_Title  = RegexTokenizer(inputCol=\"Title\",outputCol=\"title_tokenized\",pattern=\"[^a-zA-Z#+]\")\n",
    "train1 = tokenize_Title.transform(train1)\n",
    "test1 = tokenize_Title.transform(test1)\n",
    "tokenize_tags  = Tokenizer(inputCol=\"Tags\",outputCol=\"tags_tokenized\")\n",
    "train1 = tokenize_tags.transform(train1)\n",
    "remover_body_stop = StopWordsRemover(inputCol=\"body_tokenized\",outputCol=\"body_stopremove\")\n",
    "train1 = remover_body_stop.transform(train1)\n",
    "test1 = remover_body_stop.transform(test1)\n",
    "remover_title_stop = StopWordsRemover(inputCol=\"title_tokenized\",outputCol=\"title_stopremove\")\n",
    "train1 = remover_title_stop.transform(train1)\n",
    "test1 = remover_title_stop.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the unique tags in the train data\n",
    "tags =()\n",
    "for word in train1.select(\"tags_tokenized\").take(train1.count()):\n",
    "    if word not in tags:\n",
    "        tags += word\n",
    "j =1\n",
    "flat_taglist = []\n",
    "for sublist in tags:\n",
    "    for item in sublist:\n",
    "        flat_taglist.append(item)\n",
    "        j+=1\n",
    "unique_tags = list(set(flat_taglist))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the count corresponding to the unique tags\n",
    "tags_count={}\n",
    "for i in flat_taglist:\n",
    "    tags_count[i]=flat_taglist.count(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Average tags per sentence(Based on 10000 rows)\n",
    "j/train1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tried stemming the tokens but it does not impact our keywords infact it adds noise to the data with \n",
    "#wrong stemming like \"consider\" becoming'conside'\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "train3 = train1.toPandas()\n",
    "train3['body_stopremove'] = train3[\"body_stopremove\"].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1_renamed = train1.withColumnRenamed(\"tags_tokenized\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combining body and title as per the probability of 1:2 (Giving more importance to words in Title than body)\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def concat(type):\n",
    "    def concat_(*args):\n",
    "        return list(chain(*args))\n",
    "    return udf(concat_, ArrayType(type))\n",
    "\n",
    "\n",
    "concat_string_arrays = concat(StringType())\n",
    "\n",
    "#training data\n",
    "train1_concat= train1.select(concat_string_arrays(col(\"body_stopremove\"), col(\"title_stopremove\")),\"ID\",\"tags_tokenized\",\"title_stopremove\")\n",
    "train1_concat = train1_concat.withColumnRenamed(\"concat_(body_stopremove, title_stopremove)\", \"body_title\")\n",
    "train2_concat= train1_concat.select(concat_string_arrays(col(\"body_title\"), col(\"title_stopremove\")),\"ID\",\"tags_tokenized\",\"title_stopremove\")\n",
    "train2_concat = train2_concat.withColumnRenamed(\"concat_(body_title, title_stopremove)\", \"body_title\")\n",
    "\n",
    "#test data\n",
    "test1_concat= test1.select(concat_string_arrays(col(\"body_stopremove\"), col(\"title_stopremove\")),\"ID\",\"title_stopremove\")\n",
    "test1_concat = test1_concat.withColumnRenamed(\"concat_(body_stopremove, title_stopremove)\", \"body_title\")\n",
    "test2_concat= test1_concat.select(concat_string_arrays(col(\"body_title\"), col(\"title_stopremove\")),\"ID\",\"title_stopremove\")\n",
    "test2_concat = test2_concat.withColumnRenamed(\"concat_(body_title, title_stopremove)\", \"body_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting data to pandas\n",
    "train1_pd = train2_concat.toPandas()\n",
    "train1_pd= train1_pd.set_index('ID')\n",
    "\n",
    "test1_pd = test2_concat.toPandas()\n",
    "test1_pd= test1_pd.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference:https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories#comment59257073_19172087\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "\n",
    "X_train = np.array(train1_pd[\"body_title\"])\n",
    "y_train_text = np.array(train1_pd[\"tags_tokenized\"])\n",
    "\n",
    "X_test = np.array(test1_pd[\"body_title\"])\n",
    "X_test2 = np.array(train1_pd[\"body_title\"])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train_text)\n",
    "\n",
    "#fitting LinearSVC model\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=lambda doc: doc,lowercase=False)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, Y)\n",
    "predicted_test = classifier.predict(X_test)\n",
    "#tested on train data data to see the fit\n",
    "predicted_train = classifier.predict(X_test2)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "f1_score(Y, predicted, average='micro')\n",
    "#printing the current tags and predicted tags\n",
    "for item, labels in zip(y_train_text, all_labels):\n",
    "    print(item,labels)\n",
    "    \n",
    "#F1score  :0.99966386554621844\n",
    "print(\"testdata_tested\")\n",
    "print(f1_score(Y, predicted_test, average='micro'))\n",
    "print(\"traindata_tested\")\n",
    "print(f1_score(Y, predicted_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting Logistic Regression\n",
    "classifier_lr = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=lambda doc: doc,lowercase=False)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "classifier_lr.fit(X_train, Y)\n",
    "predicted_test = classifier.predict(X_test)\n",
    "#tested on train data data to see the fit\n",
    "predicted_train = classifier.predict(X_test2)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "print(\"testdata_tested\")\n",
    "print(f1_score(Y, predicted_test, average='micro'))\n",
    "print(\"traindata_tested\")\n",
    "print(f1_score(Y, predicted_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting Decision tree\n",
    "classifier_dt = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=lambda doc: doc,lowercase=False)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(tree.DecisionTreeClassifier()))])\n",
    "\n",
    "classifier_dt.fit(X_train, Y)\n",
    "predicted_test = classifier.predict(X_test)\n",
    "#tested on train data data to see the fit\n",
    "predicted_train = classifier.predict(X_test2)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "print(\"testdata_tested\")\n",
    "print(f1_score(Y, predicted_test, average='micro'))\n",
    "print(\"traindata_tested\")\n",
    "print(f1_score(Y, predicted_train, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
