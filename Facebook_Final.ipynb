{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Facebook_srossg\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('compute-0-11', 27017)\n",
    "\n",
    "db = client.FacebookChallenge_akar1\n",
    "#train data\n",
    "collection = db.fb_hw\n",
    "contents = collection.find().limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bson import json_util, ObjectId\n",
    "import json\n",
    "rddSan = json.loads(json_util.dumps(contents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(rddSan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"Body\", StringType(), True), \n",
    "                     StructField(\"Id\", IntegerType(), True), \n",
    "                     StructField(\"Tags\", StringType(), True),\n",
    "                     StructField(\"Title\", StringType(), True),\n",
    "                     StructField(\"_id\", StringType(), True)])\n",
    "\n",
    "train = sqlContext.createDataFrame(rdd, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"_id\")\n",
    "#eliminate the duplicates\n",
    "train = train.drop_duplicates(['Body','Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the relevant packages\n",
    "from pyspark.sql import SQLContext, functions as sf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler,Tokenizer, RegexTokenizer,StopWordsRemover, StringIndexer, NGram, HashingTF, IDF,MinHashLSH,MinHashLSHModel,Word2Vec,Normalizer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, OneVsRest,LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer,CountVectorizer\n",
    "from pyspark.ml.clustering import KMeans,GaussianMixture,BisectingKMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv as csv \n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:https://github.com/alexeyza/Kaggle-Facebook3/blob/master/model/pre_process.py\n",
    "#remove the HTML tags associated with the model\n",
    "def filter_html_tags(text):\n",
    "    # the following tags and their content will be removed, for example <a> tag will remove any html links\n",
    "    tags_to_filter = ['code','a']\n",
    "    if isinstance(text, unicode):\n",
    "        text = text.encode('utf8')\n",
    "    soup = BeautifulSoup(text)\n",
    "    for tag_to_filter in tags_to_filter:\n",
    "        text_to_remove = soup.findAll(tag_to_filter)\n",
    "        [tag.extract() for tag in text_to_remove]\n",
    "    return soup.get_text()\n",
    "filter_html_tags_udf = udf(filter_html_tags, StringType())\n",
    "train2 = train.withColumn(\"body\",filter_html_tags_udf(\"Body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitting the transformers\n",
    "#Regex Tokenizer to tokenize the data based on the pattern\n",
    "tokenize_body  = RegexTokenizer(inputCol=\"body\",outputCol=\"body_tokenized\", pattern=\"[^a-zA-Z#+]\")\n",
    "train1 = tokenize_body.transform(train2)\n",
    "tokenize_Title  = RegexTokenizer(inputCol=\"Title\",outputCol=\"title_tokenized\",pattern=\"[^a-zA-Z#+]\")\n",
    "train1 = tokenize_Title.transform(train1)\n",
    "tokenize_tags  = Tokenizer(inputCol=\"Tags\",outputCol=\"tags_tokenized\")\n",
    "train1 = tokenize_tags.transform(train1)\n",
    "remover_body_stop = StopWordsRemover(inputCol=\"body_tokenized\",outputCol=\"body_stopremove\")\n",
    "train1 = remover_body_stop.transform(train1)\n",
    "remover_title_stop = StopWordsRemover(inputCol=\"title_tokenized\",outputCol=\"title_stopremove\")\n",
    "train1 = remover_title_stop.transform(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the unique tags in the train data\n",
    "tags =()\n",
    "for word in train1.select(\"tags_tokenized\").take(train1.count()):\n",
    "    if word not in tags:\n",
    "        tags += word\n",
    "j =1\n",
    "flat_taglist = []\n",
    "for sublist in tags:\n",
    "    for item in sublist:\n",
    "        flat_taglist.append(item)\n",
    "        j+=1\n",
    "unique_tags = list(set(flat_taglist))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the count corresponding to the unique tags\n",
    "tags_count={}\n",
    "for i in flat_taglist:\n",
    "    tags_count[i]=flat_taglist.count(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average tags per sentence(Based on 1000 rows)\n",
    "str(j/train1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tried stemming the tokens but it does not impact our keywords infact it adds noise to the data with \n",
    "#wrong stemming like \"consider\" becoming'conside'\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "train3 = train1.toPandas()\n",
    "train3['body_stopremove'] = train3[\"body_stopremove\"].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1_renamed = train1.withColumnRenamed(\"tags_tokenized\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combining body and title as per the probability of 1:2 (Giving more importance to words in Title than body)\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def concat(type):\n",
    "    def concat_(*args):\n",
    "        return list(chain(*args))\n",
    "    return udf(concat_, ArrayType(type))\n",
    "\n",
    "\n",
    "concat_string_arrays = concat(StringType())\n",
    "\n",
    "#training data\n",
    "train1_concat= train1.select(concat_string_arrays(col(\"body_stopremove\"), col(\"title_stopremove\")),\"ID\",\"tags_tokenized\",\"title_stopremove\")\n",
    "train1_concat = train1_concat.withColumnRenamed(\"concat_(body_stopremove, title_stopremove)\", \"body_title\")\n",
    "train2_concat= train1_concat.select(concat_string_arrays(col(\"body_title\"), col(\"title_stopremove\")),\"ID\",\"tags_tokenized\",\"title_stopremove\")\n",
    "train2_concat = train2_concat.withColumnRenamed(\"concat_(body_title, title_stopremove)\", \"body_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+\n",
      "|          body_title| ID|      tags_tokenized|    title_stopremove|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "|[aloha, everyone,...| 42|[php, firefox, sa...|[safari, displays...|\n",
      "|[specified, actio...|155|[asp.net-mvc, asp...|[graceful, degrad...|\n",
      "|[implement, solut...|736|[asp.net, sql, as...|[working, date, r...|\n",
      "|[php, ini, file, ...|850|[php, unicode, lo...|[internal, encodi...|\n",
      "|[friend, running,...|157|[windows-vista, r...|[repair, corupt, ...|\n",
      "|[im, using, array...|793|   [java, arraylist]|[arraylist, objec...|\n",
      "|[make, applicatio...|332|[java, android, a...|[make, applicatio...|\n",
      "|[following, guide...|857|[java, refactorin...|  [smaller, methods]|\n",
      "|[looking, good, r...|464|[windows, securit...|[good, resource, ...|\n",
      "|[checked, many, a...|621|[xml, xslt, names...|[add, namespace, ...|\n",
      "|[deleting, event,...|173|[development, eve...|[cyclic, calling,...|\n",
      "|[m, running, serv...|820|[mysql, distribut...|[distributed, mys...|\n",
      "|[m, trying, use, ...| 84|[java, ruby, wsdl...|[ruby, savon, com...|\n",
      "|[using, get, prog...|929|[c#, winforms, re...|[getting, exe, na...|\n",
      "|[machine, initial...| 56|   [ntfs, filenames]|[create, name, ex...|\n",
      "|[m, frequency, an...|915|[c++, mp3, m4a, f...|[mp, m, frequency...|\n",
      "|[m, using, websph...|964|[websphere, datas...|[auto, generated,...|\n",
      "|[webapp, user, cl...|566|[javascript, jque...|[something, wrong...|\n",
      "|[spring, configur...|509|      [java, spring]|[performance, dif...|\n",
      "|[background, capi...|580|        [capistrano]|[capistrano, mult...|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train2_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train2_concat\n",
    "hashingTF = HashingTF(inputCol=\"body_title\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(train2_concat)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"idf_features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "normalizer = Normalizer(inputCol=\"idf_features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(rescaledData)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"normFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(l1NormData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 2242342.85745\n",
      "Cluster Centers: \n",
      "[ 0.00640124  0.02984128  0.         ...,  0.          0.04480871\n",
      "  0.06307919]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.          0.26829518  0.         ...,  0.          0.          0.        ]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "Within Set Sum of Squared Errors = 2242342.85745\n"
     ]
    }
   ],
   "source": [
    "#Performing Kmeans clustering\n",
    "kmeans = KMeans().setK(10).setSeed(1)\n",
    "model = kmeans.fit(output)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(output)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    \n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "transformed = model.transform(output).select(\"prediction\",\"body_title\",\"tags_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster Sizes:[971, 1, 18, 1, 1, 1, 4, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "summary= model.summary\n",
    "#Show the summary predictions\n",
    "print(\"cluster Sizes:\"+str(summary.clusterSizes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture().setK(10).setSeed(232)\n",
    "gmm_cluster_model = gmm.fit(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLikelihood of a cluster = 32173.2276132\n",
      "Cluster Sizes = [141, 32, 58, 93, 204, 104, 102, 155, 73, 38]\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|mean                                                                                                                                                                                                                                                                                                                                                                                                                     |cov                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0433650970183399,0.04116252323070106,0.06311513346759348,0.053275948709178685,0.044835970668601635,0.08683986750056472,0.05931081005571029,0.035493941051831984,0.04588130848846298,0.04318845143214168,0.03338018527037125,0.041299370156626276,0.05731065642481811,0.043220912729515205,0.04882842668847817,0.040524343007836604,0.0790853278666268,0.04908815493163292,0.05477397824390236,0.03601959305706578]    |8.493218173196892E-4    -1.4623304330338543E-4  ... (20 total)\n",
      "-1.4623304330338543E-4  8.21721834655371E-4     ...\n",
      "-1.482447733444434E-4   1.1477264557542248E-7   ...\n",
      "-8.826278274347646E-6   -4.5953176715742454E-5  ...\n",
      "1.0786437029897433E-4   -1.3870721044211367E-4  ...\n",
      "-5.257673129242488E-4   -2.566692363806404E-5   ...\n",
      "-1.4346407409913795E-4  2.357316266860207E-4    ...\n",
      "1.629279465626867E-5    -2.0967678144797434E-4  ...\n",
      "-1.2803998554396733E-4  -8.609762547115469E-5   ...\n",
      "-4.9786821730826254E-5  -6.348245991141971E-5   ...\n",
      "8.358404887108153E-5    -8.155667359035224E-5   ...\n",
      "6.244301598083212E-5    -1.8926045175464936E-5  ...\n",
      "-1.3139725978026992E-4  -1.4410566729076009E-4  ...\n",
      "3.5409280540832074E-5   1.7791924107750091E-4   ...\n",
      "7.252550295282685E-5    -6.481532430387104E-5   ...\n",
      "1.8476118043391206E-4   1.661571276638017E-4    ...\n",
      "-1.2998052572087238E-4  -2.685956522220438E-4   ...\n",
      "-5.579726822665039E-5   4.486710583360827E-6    ...\n",
      "-4.485695313609832E-5   -1.1660879767730608E-4  ...\n",
      "1.0019228502983421E-4   4.29406787802127E-6     ...|\n",
      "|[0.023000764017944174,0.041884309787859425,0.03360459449121782,0.02891956553622421,0.029541311477027266,0.032964798571036395,0.03661590799390012,0.048126058858204326,0.06062218010171555,0.042244591119793014,0.04434761799293147,0.08133153733343705,0.07723070742615672,0.05291806966711484,0.08969253970509611,0.05679744197761124,0.10403501205770548,0.033054470032373814,0.021833728257334508,0.06123479359531645]|6.365052567494148E-4    -5.779509670546244E-5   ... (20 total)\n",
      "-5.779509670546244E-5   0.002040062666289799    ...\n",
      "-1.0364271374489208E-4  8.385505806401471E-6    ...\n",
      "1.7514471099923577E-4   5.146980807380629E-4    ...\n",
      "1.483112025317949E-4    -2.5908902137262345E-4  ...\n",
      "-9.563073570244262E-5   -4.541258364812601E-4   ...\n",
      "-2.743195519081207E-5   1.9158070298513943E-4   ...\n",
      "1.4647440418759115E-4   -7.19499652605021E-4    ...\n",
      "3.3734122855256214E-4   1.9716098163179477E-4   ...\n",
      "-2.881586334085948E-5   -1.8389846898945222E-4  ...\n",
      "1.2443402829047628E-4   0.0010457644868183862   ...\n",
      "-4.60090258231634E-4    -0.0012818438253647056  ...\n",
      "-3.266453842862387E-4   -6.76832555696244E-4    ...\n",
      "-2.176699590571467E-4   7.268345884487185E-4    ...\n",
      "-2.705505341838991E-4   -3.04098048197875E-4    ...\n",
      "9.81478427539253E-5     6.855664952447359E-4    ...\n",
      "-2.6277398285354546E-4  -4.0453962253543963E-4  ...\n",
      "-7.177483276277479E-5   -2.296328144184179E-4   ...\n",
      "-5.935467837747835E-5   1.2559123312962187E-4   ...\n",
      "3.1581732037218844E-4   -9.642897987261693E-4   ...|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "clustermodel weights:[0.1384313685051289, 0.032298485492640215, 0.0578938690132506, 0.09339902334706908, 0.20073442816436005, 0.10437806054471149, 0.10348463797046326, 0.15628016684105073, 0.07486426399301252, 0.03823569612831301]\n"
     ]
    }
   ],
   "source": [
    "#model parameters\n",
    "summary_gmm= gmm_cluster_model.summary\n",
    "print(\"LogLikelihood of a cluster = \" + str(summary_gmm.logLikelihood))\n",
    "print(\"Cluster Sizes = \" + str(summary_gmm.clusterSizes))\n",
    "gmm_cluster_model.gaussiansDF.show(2,truncate=False)\n",
    "#weights for each of the cluster\n",
    "print (\"clustermodel weights:\" +str(gmm_cluster_model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|probability                                                                                                                                                                                                            |prediction|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[2.8877506806219266E-12,7.376330270715967E-31,9.356904868196832E-12,0.5526827019482105,2.8340717397983486E-5,3.745006874085722E-30,0.004156699982355203,0.4431184516408654,1.3805698926122665E-5,7.376330270352109E-31]|3         |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View prediction and probability of belonging to each of the cluster\n",
    "transformed_output= gmm_cluster_model.transform(output).select(\"prediction\",\"body_title\",\"tags_tokenized\",\"probability\")\n",
    "transformed_output.select(\"probability\",\"prediction\").show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "number\n",
      "input\n",
      "n\n",
      "links\n",
      "post\n",
      "authentication\n",
      "got\n",
      "product\n",
      "clone\n",
      "happens\n",
      "\n",
      "\n",
      "Topic1:\n",
      "using\n",
      "m\n",
      "like\n",
      "code\n",
      "get\n",
      "use\n",
      "want\n",
      "way\n",
      "file\n",
      "one\n",
      "\n",
      "\n",
      "Topic2:\n",
      "statement\n",
      "im\n",
      "windows\n",
      "wondering\n",
      "problems\n",
      "pass\n",
      "end\n",
      "instance\n",
      "needs\n",
      "strange\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reference:https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "\n",
    "docDF = train2_concat.select(\"ID\",\"body_title\")\n",
    "Vector = CountVectorizer(inputCol=\"body_title\", outputCol=\"vectors\")\n",
    "cv = Vector.fit(docDF)\n",
    "output = cv.transform(docDF)\n",
    "\n",
    "topics_bag = output.select(\"ID\", \"vectors\").rdd.map(lambda (x,y): [x,Vectors.fromML(y)]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(topics_bag, k=3,maxIterations=100,optimizer='online')\n",
    "topics = ldaModel.topicsMatrix()\n",
    "vocabArray = model.vocabulary\n",
    "\n",
    "wordNumbers = 10  # number of words per topic\n",
    "topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "\n",
    "topics_final = topicIndices.map(lambda topic: topic_render(topic)).collect()\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic\" + str(topic) + \":\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
